
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Def., Theorem, Proof, etc. ---------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\newenvironment{proof}           {\noindent{\bf Proof.} }%
                                 {\null\hfill$\Box$\par\medskip}
%--------------- Algorithm --------------------------------------------------
\newtheorem{algX}{Algorithm}
\newenvironment{algorithm}       {\begin{algX}\begin{em}}%
                                 {\par\noindent --- End of Algorithm ---
                                 \end{em}\end{algX}}
\newcommand{\step}[2]            {\begin{list}{}
                                  {  \setlength{\topsep}{0cm}
                                     \setlength{\partopsep}{0cm}
                                     \setlength{\leftmargin}{0.8cm}
                                     \setlength{\labelwidth}{0.7cm}
                                     \setlength{\labelsep}{0.1cm}    }
                                  \item[#1]#2    \end{list}}
                                 % usage: \begin{algorithm} \label{xyz}
                                 %        ... \step{(1)}{...} ...
                                 %        \end{algorithm}
%--------------- Figures ----------------------------------------------------
\usepackage{graphicx}

\newcommand{\includeFig}[3]      {\begin{figure}[htb] \begin{center}
                                 \includegraphics
                                 [width=4in,keepaspectratio] %comment this line to disable scaling
                                 {#2}\caption{\label{#1}#3} \end{center} \end{figure}}
                                 % usage: \includeFig{label}{file}{caption}
                                 
\usepackage[inline]{enumitem}

% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{LITERATURE REVIEW: GPU-powered outlier detection on stream data}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
Kangqing YU\\
School of Computer Science\\
Carleton University\\
Ottawa, Canada K1S 5B6\\
{\em kangqingyu@cmail.carleton.ca}
% ############################################################################
} % end-authors
% ############################################################################

\maketitle



% ############################################################################
\section{Introduction} \label{intro}
% ############################################################################

In recent years, parallel computing has drawn a tremendous amount of attentions and it is becoming a main stream for many applications including data science, biochemistry, medical etc. Among all different categories of parallel computing, one which is really easy to achieve from the hardware perspective is GPU(Graphics Processing Units) programming, which takes advantages of Graphical Processor Unit to accelerate the performance for many tasks which are considered computational expensive in CPU(Central Processing Unit). 
However, due to the nature of SIMD\footnote{Single Instruction Multiple Data} in GPU, programming in GPU can be very challenging for developers and therefore may incur a deep learning curve. 

In this project, I use GPU programming power to solve a specific kind of data mining task, called outlier detection(or anomaly detection) to address the increasing challenge in solving this problem in the dynamic streaming environment. An outlier in a dataset is a data point that is considerably different from the rest of the data as if it is generated by a different mechanism\cite{7516110}. Applications of outlier detections vary in numerous fields, including fraud detection, network intrusion detection, medical image screening, environment monitoring etc. A stream environment is where data come constantly at a high volume and may change over time. This can impose a very high requirement for computation power as decisions need to be made in real time within limited time among all data. In this project, I use \textbf{incremental kernel density estimation} approach, where an outlier decision is made not only based the data points in the current data stream, but also taking consideration of historical data without the necessity to store the entire dataset in the secondary memory. The performance of the result is compared with the same method that runs sequentially on CPU to further illustrate that GPU can practically be used in a streaming environment to detect outliers at a high rate of data volume where it is otherwise incapable to handle by CPU. 


% ############################################################################
\section{Literature Review} \label{litrev}
% ############################################################################

A lot of techniques have been introduced in last decades to solve the outlier detection problem. And these techniques can be briefly summarized into three different categories: 

\begin{enumerate}
  \item Supervised approaches
  \item Semi-supervised approaches
  \item Unsupervised approaches
\end{enumerate}

Supervised approaches use techniques from machine learning and typically require building a prediction model for rare events based on manually labelled data(the training set), and use it to classify new event based on the learnt model\cite{Joshi:2001:MNH:376284.375673,sup02}. In other words, the outlier problem in this case becomes a classification problem where we are only interested in the minority class whose data deviate largely from the rest. This can somehow be a overkill because minority can often be detected based on the data distributions and patterns rather than manually labelled data. The main drawback of using supervised methods is that it fails to capture the new treading of the outliers, and also assuming the underlying distribution of the data is known. However, supervised approaches may give guaranteed performance when the data distribution is static. A lot of supervised learning methods exists that work quite well to detect outliers, including Support Vector Machines, Neural Network, K-Mean and KNN etc. 

Another problem of supervised approaches is that labelled data is really hard to generate, and in most cases, it is impractical for outlier data. Semi-supervised learning\cite{Basu:2004:PFS:1014052.1014062,semi-sup02} combines labeled and unlabeled data during training to improve performance. In semi-supervised learning, some labeled data are used with unlabeled data to obtain better clustering. It is known that applying semi-supervised learning to anomaly detection can improve the detection accuracy\cite{Yu2009}. One approach introduced by Jing Gao et all.\cite{Gao:2006:SOD:1141277.1141421} takes advantage of K-mean clustering in unsupervised learning by adding penalties to the objective function for mislabelled data points and optimize the overall objective function. 

Although some of those techniques may generate very promising results, they work well only in static data and typically don't fit into the context of dynamic streaming environment. In other words, both supervised and semi-supervised methods will assume that they will have \textbf{random access} to the underlying data while this is not possible for streaming data when you can only have portion of it at one time. The definition of data stream is as follows:

\begin{definition}
\textbf{Data Stream:} A data stream is a possible infinite series of data points ..., $o_{n-2}$ , $o_{n-1}$, $o_{n}$ , ..., where data point $o_{n}$ is received at time $o_{n}.t$.
\end{definition}

Based on the nature of data stream, when building an application that process data stream, these three key characteristics of data stream need to be taken into consideration: uncertainty, transiency, and incompleteness\cite{Sadik:2011:OOD:2076623.2076635}. Uncertainty means the data distribution of the model may change over the time as new data comming in in a unpredictible way. This term is sometimes known as \textbf{concept drift} in some literatures. Dealing with concept drift is a main challenge in most streaming applications. Transiency means it is not possible to store the entire dataset from data stream in the memory. The data can only be accessed in one pass and when it is processed, it should be deleted from memory. Completeness assume that the data will come indefinitely and they will never stop. Outlier detection in data stream is very challenging. Suppose you have a data stream that keeps coming indefinitely, at one time $t_{i}$, you identify object $o_{k}$ as being outlierness in the current window. And after some time $W$ at time $t{i} + W$ when the whole recent history is considered, object $o_{k}$ may become an inliner.  And vice versa. This can be illustrate in Figure \ref{fig:evolving}

\includeFig{fig:evolving}{Figures/evolving-data}{Evolving 2D data stream.}

In contrast, unsupervised learning methods don't require labelled input and typically don't assume a fixed data distribution as the model l can be dynamically built based on variations of data. Many best-known techniques of outlier detection fall into this category and based on the context, they can mostly be classified into two categories: \textbf{Unsupervised outlier detection on static data} and \textbf{Unsupervised outlier detection on streaming data}. 

\textbf{Distance-based outlier detection on static data} was first introduced by Knorr and Ng\cite{EKnorr:1998}. It calculates the pair-wise Euclidian Distance between all data and if one data point has less than $k$ neighbors within distance $R$, it is considered an outlier. The formal definition of it consists of the following:

\begin{definition}
\textbf{Neighbor:} Given a distance threshold R(R $>$ 0), a data point $p_{i}$ is a neighbor of datapoint $p_{j}$ if the distance between $p_{i}$ and $p_{j}$ is not greater than R. A data point is not considered a neighbor of itself.
\end{definition}

\begin{definition}
\textbf{Distant-based outlier:} Given a dataset D, a count threshold k (k $>$ 0) and a distance threshold R (R $>$ 0), a distance-based outlier in D is a data point that has less than k neighbors in D
\end{definition}

There are some variants of the static distance-based approaches, and their ideas are quite similar. For instance, Ramaswamy et all.\cite{Ramaswamy:2000:EAM:342009.335437} purposed a method where an outlier is defined by considering the total number of objects whose distance to its $k^{th}$ nearest neighbor is smaller than itself. Angiulli and Pizzuti\cite{1377172} introduced a method where an outlier is defined by taking into account of the sum of the distances from $1^st$ up to the $k^{th}$ nearest neighbors. These methods are sometimes referred as KNN and it should be noted that it is different from the term KNN in supervised machine learning. 

\textbf{Density-based approach on static data} is another way to detect outlier on static data. The basic idea is to assign a degree of being outlier(a score) based on the density of local neighbourhood, given some predefined restrictions. A popular example of this approach is Local Outlier Factor(LOF) algorithm\cite{Breunig:2000:LID:342009.335388}, which is defined as follows: Let \textbf{k-distance(q)} denote as distance to the $k_{th}$ nearest neighbor of q, then

\begin{equation}
\textbf{reach-distk(q,p)} = max(d(q,p), k-distance(p))
\end{equation}

where d(q,p) is Euclidean distance from q to p  

\begin{equation}
\textbf{lrd(q)} =  \frac{1}{\sum\limits_{k \in knn(q)} reach-distk(q,p) / k}
\end{equation}

where \textbf{lrd(q)} is the local reachability density of data point q

And finally \textbf{LOF} of data point q is defined as:

\begin{equation}
 LOF(q) = \frac{\frac{1}{k} \sum_{k \in knn(q)} lrd(q)}{lrd(q)}
\end{equation}

Data records (points) with high LOF have local densities smaller than their neighborhood and typically represent stronger outliers, unlike data points belonging to uniform clusters that usually tend to have lower LOF values\cite{4221341}

Both distance-based and density-based outlier detection techniques have a very wide range of applications, but one of their most notorious drawbacks is that they are usually computational expensive, especially in the scenario when the input data is high dimensional and a metric function is used to calculate the pair-wise distances.  

\textbf{Statistics approach} on the other hand, is another way to perform outlier detection on data with random access without requiring expensive computational resources. It is based on the probability theory and normally models the underlying data using a stochastic distribution(e.g. Gaussian distribution). One of the most popular one used is \textbf{auto-regression} model or sometimes being referred as Gaussian mixture model\cite{4438332}. It is very popular for time series outlier detection and its definition is given by 

\[ x(t) = a_{1}(t) \times x(t-1) + ... + a_{n}(t) \times x(t -n) + \xi(t) \]

where $\xi(t)$ is the noise and is almost always assumed to be a Gaussian white noise. Based on this formula, we can estimate the coefficient parameters $a_{i}(t)$ based on the given time series of $x(t),...x(t-n)$. The model can then be used to predict future time series by defining a threshold, called cut-off limit and the data point is identified as an outlier if it is beyond this threshold.

\textbf{Deviation} is another way of statically outlier detection first introduced by Arning et al.\cite{A.Arning:deviation}, where an outlier is detected if feature space of one data point deviates largely from other data points. Aggarwal and YU\cite{Aggarwal:2001:ODH:375663.375668} proposed a technique for outlier detection. The basic idea in their definition is, a point is an outlier, if in some lower dimensional projection it is present in a local region of abnormally low density. This method is also an efficient method for high dimensional data set\cite{04666541}. Another technique introduced by Harkins et al.\cite{S.Harkins:rnn} is to take advantage of replicator neutral network(RNN) to detect outliers. There might be other techniques used for unsupervised outlier detection but due to the limitation of this paper, I can not list all of them. The ones mentioned above are those best-known so far.

As modern applications have an increasing demands to process streaming data in real-time, a lot of these static methods mentioned before have been extended to work in the dynamic streaming environments. The all based on the same ideas in static approaches but algorithms have been modified in an incremental fashion to address the \textbf{concept drift} of the data stream properties.

Distance-based outlier detection approach was among the first which start to apply the method in the streaming context. In the last decade, there are several studies which focus on \textbf{distance-based outlier detection in data streams(DODDS)}. Due to the fact that the distance-based require random access on the data and this is not possible with stream data, \textit{sliding window} technique was introduced which only keep a number of active objects in current window. When objects expire, they are deleted from memory as new object comes in. There are two window models in data streams: count-based window and time-based window which are defined as follows: 

\begin{definition}
 \textbf{Count-based window:} Given data point $o_{n}$ and a fixed window size $W$, the count-based window $D_{n}$ is the set of $W$ data points: $o_{n - W+1}$,$o_{n -W+2}$,..., $o_{n}$
\end{definition}

\begin{definition}
 \textbf{Time-based window:} Given data point $o_{n}$ and a time period $T$ , the time-based window $D(n, T)$ is the set of $W_{n}$ data points: $o_{n'}$,$o_{n'+1}$,...,$o_{n}$ with $W_{n} = n-n' +1$ and $o_{n}.t-o_{n'}.t = T$.
\end{definition}

Numerous algorithms have been invented to process stream data using sliding window on outlier detection. And based on the benchmark among all DODDS algorithms given by Luan Tran et al.\cite{Tran:2016:DOD:2994509.2994526}, the MCOD algorithm introduced by M.Kontaki et al.\cite{5767923} seems to have the most satisfying performance. Its basic idea is to pre-compute the \textit{safe inliers} that have more than $k$ neighbors which arrived after $p_{i}$ by using an event queue, which can reduce greatly on space complexity. Because the neighbors which arrives before $p_{i}$ may expire, by declaring the neighbors which arrived after $p_{i}$ to be larger than $k$, we can safely mark $p_{i}$ as inlier. The time complexity of this algorithm is guaranteed to be $O(n\log{k})$ while maintaining the space complexity to be $O(nk)$

Most of DODDS algorithms are based on the original definition of distance-based technique given by Knorr and Ng\cite{EKnorr:1998}. The other distance-based techniques in outlier detection such as KNN remain unsolved in the streaming context. It would be interested to see if those methods can be extended to work in the data streaming context. And since they are based on \textit{sliding window}, only active objects in current window are considered. And therefore, these methods lack a global view of the entire dataset and sometimes, this may affect accuracy. 

Clustering is another technique to outlier detection on stream data. Since clustering is a technique in unsupervised machine learning, it inspired the ideas of outlier detection in streaming environment. Two main algorithms exists for clustering-based approaches. One of them is called \textbf{K-Mean clustering}\cite{04666541}. It divides the stream into chunks and cluster chunks using k-mean clustering into fixed number of $k$ clusters. The mean of each clusters is calculated by metrics information of all data points in a cluster. If a data point is too far from the mean of its data point by a threshold, it will be considered as a candidate outlier. Both the \textit{mean} and the \textit{candidate outliers} detected in this chunk are carried over to next chunk in stream to further compare with data in other chunks. Other data in this chunk is simply deleted from memory. If the candidate outlier passed a given number of chunks, it is then identified as \textit{real outlier}. Compared to K-Mean clustering, \textbf{K-Median clustering}\cite{DBLP:journals/corr/abs-1002-4003} clusters each chunk of data into variable number of clusters(from $k$ to $k log(n)$), and different from K-mean clustering, it passes the weighted medians found in current chunk into next chunk for testing outlierness rather than the mean and candidate outliers. Both of these two approaches will require users' input of value $k$ but K-Median clustering theoretically is better since the number of clusters is not fixed. 

To address this problem of storage and users' input parameters, an ideal method need to find a efficient way to efficiently mine its historical data or gradually fade old data away without users' intervention. An technique inspired from \textit{sensor network} is mentioned in\cite{Subramaniam:2006:OOD:1182635.1164145}, where it use a \textbf{kernel density estimator} to model the distribution of the sensor data. When used for outlier detection, the number of neighbors of a given data point $p_{i}$ is estimated by the distribution function $f(p_{i})$. In \cite{4221341}, D. Pokrajac et al illustrated that the LOF algorithm can be applied incrementally and the insertion of new data as well as deletion of obsolete data does not depend on the total number of $N$ in dataset and therefore the time complexity of incremental LOF algorithm can theoretically be $O(N\log{N})$. These two methods are both based on computing of the \textbf{densities of local neighborhood}. However, there are still a lot of noise around these algorithms and many researchers argue that it is still computational expensive in practice. 

Even though there are many studies in the last decade that focusing on detecting outlier online in data stream, only a few tried to solve this problem using a parallel implementation. In \cite{7516110} C. HewaNadungodage et al.implemented a so-called SOD\_GPU\footnote{Stream Outlier Detector-GPU} algorithm which is based on kernel density estimator powered by GPU to effectively detect outlier in continuous data streams. The results seems very promising as it is 20X faster compared to a multicore CPU implementation and even a higher accuracy rate compared to the sliding window approach. And this is the only work, that I have found which tried to solve the outlier detection problem in streaming environment using parallel computing approach by the time of writing. Other parallel implementations for outlier detection exist but they only work in a static fashion, in which case, it does not seem to be quite necessary. In \cite{6641405},  Angiulli et al. proposed a distance-based KNN algorithm powered by GPU and similarly, Matsumoto and Hung\cite{Matsumoto2012} introduced a GPU-accelerated approach to detect the outliers in uncertain data. Another GPU-accelerated approach to detect outlier in static data was purposed in \cite{Alshawabkeh:2010:ALO:1735688.1735707}, where the LOF(Local outlier factor) algorithm is used. Anna Koufakou et all.\cite{4634266} developed a parallel outlier detection strategy based on \textbf{Attribute Value Frequency(AVF)}\cite{4410382}\footnote{Note that the AVF algorithm can only detect outlier in categorical data.} algorithm using MapReduce programming paradigm. Other incremental parallel methods exist for online outlier detection but they either require storing the entire history of data stream or giving a set of user-defined parameters, which is hard to define in most cases. 

In this paper, I am only interested in applying parameterless-parallel algorithms in dynamic streaming environment without imposing a large storage requirements to detect outliers. Many of the algorithms mentioned pose a harsh computational requirements and it would be worth investigating into those techniques to see if these algorithms are parallelizable. In fact, a lot of those algorithms such as sliding window was invented in order to lower the hardware requirement so it can be run on CPU or in sequential. In my case, I want to use Graphical Processing Unit(GPU) and CUDA framework as tools for implementing parallelism because firstly, it is more powerful than the multiple-core processors. And secondly, it is easier to set-up compared to clusters where configurations are required on network level and usually fault-tolerance need to be considered for distributed system. In addition, CUDA has been well-established and is a well-supported library in community. One can easily find helps if he needs to.


% ############################################################################
% Bibliography
% ############################################################################
\bibliographystyle{plain}
\bibliography{my-bibliography}     %loads my-bibliography.bib

% ============================================================================
\end{document}
% ============================================================================
