
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Def., Theorem, Proof, etc. ---------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\newenvironment{proof}           {\noindent{\bf Proof.} }%
                                 {\null\hfill$\Box$\par\medskip}
%--------------- Algorithm --------------------------------------------------
\newtheorem{algX}{Algorithm}
\newenvironment{algorithm}       {\begin{algX}\begin{em}}%
                                 {\par\noindent --- End of Algorithm ---
                                 \end{em}\end{algX}}
\newcommand{\step}[2]            {\begin{list}{}
                                  {  \setlength{\topsep}{0cm}
                                     \setlength{\partopsep}{0cm}
                                     \setlength{\leftmargin}{0.8cm}
                                     \setlength{\labelwidth}{0.7cm}
                                     \setlength{\labelsep}{0.1cm}    }
                                  \item[#1]#2    \end{list}}
                                 % usage: \begin{algorithm} \label{xyz}
                                 %        ... \step{(1)}{...} ...
                                 %        \end{algorithm}
%--------------- Figures ----------------------------------------------------
\usepackage{graphicx}

\newcommand{\includeFig}[3]      {\begin{figure}[htb] \begin{center}
                                 \includegraphics
                                 [width=4in,keepaspectratio] %comment this line to disable scaling
                                 {#2}\caption{\label{#1}#3} \end{center} \end{figure}}
                                 % usage: \includeFig{label}{file}{caption}
                                 
\usepackage[inline]{enumitem}

% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{LITERATURE REVIEW: GPU-powered outlier detection on stream data}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
Kangqing YU\\
School of Computer Science\\
Carleton University\\
Ottawa, Canada K1S 5B6\\
{\em kangqingyu@cmail.carleton.ca}
% ############################################################################
} % end-authors
% ############################################################################

\maketitle



% ############################################################################
\section{Introduction} \label{intro}
% ############################################################################

In recent years, parallel computing has drawn a tremendous amount attentions and it is becoming a main stream for many applications including data science, biochemistry, medical etc. Among all different categories of parallel computing, one which is really easy to achieve from the hardware perspective is GPU programming, CUDA to be specific, which takes advantages of Graphical Processor Unit to accelerate the performance for many tasks which are considered computational expensive. However, due to the nature of SIMD\footnote{Single Instruction Multiple Data} in GPU programming, programming in frameworks such as CUDA can be very challenging and therefore may incur a deep learning curve. 

In this project, I want to use GPU programming power to solve a specific kind of data mining task, called outlier detection(or anomaly detection) to address the increasing challenge in solving this problem in the streaming environment. An outlier in a dataset is a data point that is considerably different from the rest of the data as if it is generated by a different mechanism\cite{7516110}. Applications of outlier detections vary in numerous fields, including fraud detection, network intrusion detection, medical image screening, environment monitoring etc. A stream environment is where data could come constantly at a high volume and may change over time. This can impose a very high requirement of computation power as decisions need to be made in real time within limited time period among all data. Specifically, I use \textbf{incremental kernel density estimation approach}, where an outlier decision is made not only based the data points in the current data stream, but also taking consideration of historical data without the necessity to store the entire dataset in the secondary memory. The accuracy of the result is compared with the traditional approach where data is analyzed in a static environment to further illustrate that GPU can definitely be used in a streaming environment to facilitate outlier detection at a high rate of data volume where static approaches can not achieve otherwise.


% ############################################################################
\section{Literature Review} \label{litrev}
% ############################################################################

A lot of techniques have been introduced in last decades to solve the outlier detection problem. And these techniques can be briefly summarized into three different categories: 

\begin{enumerate}
  \item Supervised approaches
  \item Semi-supervised approaches
  \item Unsupervised approaches
\end{enumerate}

Supervised approaches use techniques from machine learning and typically require building a prediction model for rare events based on manually labelled data(the training set), and use it to classify new event based on the learnt model\cite{Joshi:2001:MNH:376284.375673,sup02}. In other words, the outlier problem in this case becomes a classification problem where we are only interested in the minority class whose data deviate largely from the rest. This can somehow be a overkill because minority can often be detected based on the data distributions and patterns rather than manually labelled data. The main drawback of using supervised methods is that it fails to capture the new treading of the outliers, and also assuming the underlying distribution of the data is known. However, supervised approaches may give guaranteed performance when the data distribution is static. A lot of supervised learning methods exists that could work quite well, including Support Vector Machines, Neural Network, K-Mean and KNN etc. 

Another problem of supervised approaches is that labelled data is really hard to generate, and in most cases, it is impractical for outlier data. Semi-supervised learning\cite{Basu:2004:PFS:1014052.1014062,semi-sup02} combines labeled and unlabeled data during training to improve performance. In semi-supervised clustering, some labeled data are used with unlabeled data to obtain better clustering. It is known that applying semi-supervised learning to anomaly detection can improve the detection accuracy\cite{Yu2009}. One approach introduced by Jing Gao\cite{Gao:2006:SOD:1141277.1141421} takes advantage of K-mean clustering in unsupervised learning by adding penalties to the objective function for mislabelled data points and optimize the overall objective function. 

Although some of those techniques may generate very promising results, they work well only in static data and typically don't fit into the context of dynamic streaming environment. In other words, both supervised and unsupervised methods will assume that it will have \textbf{random access} to the underlying data while this is not possible for streaming data when you can only have portion of it at one time. The definition of data stream is as follows:

\begin{definition}
\textbf{Data Stream:} A data stream is a possible infinite series of data points ..., $o_{n?2}$ , $o_{n?1}$, $o_{n}$ , ..., where data point $o_{n}$ is received at time $o_{n}.t$.
\end{definition}

Based on the nature of data stream, when building an application that process data stream, these three key characteristics of data stream need to be taken into consideration: uncertainty, transiency, and incompleteness\cite{Sadik:2011:OOD:2076623.2076635}. Uncertainty means the data distribution of the model may change over the time as new data comming in in a unpredictible way. This term is sometimes known as \textbf{concept drift} in some literatures. Transiency means it is not possible to store the entire dataset from data stream in the memory. The data can only be accessed in one pass and when it is processed, it should be deleted from memory. Completeness just assume that the data will come indefinitely and they will never stop. This will create a really huge challenge for outlier detection as suppose you have data stream that keep comming it. At one time $t_{i}$, you identify object $o_{k}$ as being outlierness in the current window. And after some time $W$ at time $t{i} + W$ when the whole recent history is considered, object $o_{k}$ may become an inliner.  And vice versa. This can be illustrate in Figure \ref{fig:evolving}

\includeFig{fig:evolving}{Figures/evolving-data}{Evolving 2D data stream.}

In contrast, unsupervised learning methods don't require labelled input and typically don't assume a fixed data distribution as the model l can be dynamically based on different data. Many best-known techniques of outlier detection fall into this category and based on the context, they can mostly be classified into two categories: \textbf{Unsupervised outlier detection on static data} and \textbf{Unsupervised outlier detection on streaming data}. 

\textbf{Distance-based outlier detection on static data} was first introduced by by Knorr and Ng\cite{EKnorr:1998}. It calculates the pair-wise Euclidian Distance between all data and if one data point has less than $k$ neighbors within distance $R$, it is considered an outlier. The formal definition of it consists of the following:

\begin{definition}
\textbf{Neighbor:} Given a distance threshold R(R $>$ 0), a data point $p_{i}$ is a neighbor of datapoint $p_{j}$ if the distance between $p_{i}$ and $p_{j}$ is not greater than R. A data point is not considered a neighbor of itself.
\end{definition}

\begin{definition}
\textbf{Distant-based outlier:} Given a dataset D, a count threshold k (k $>$ 0) and a distance thresh- old R (R $>$ 0), a distance-based outlier in D is a data point that has less than k neighbors in D
\end{definition}

There are some variants of the static distance-based approaches, and their ideas are quite similar. For instance, Ramaswamy\cite{Ramaswamy:2000:EAM:342009.335437} purposed a method where an outlier is defined by considering the total number of objects whose distance to its $k^{th}$ nearest neighbor is smaller than itself. Angiulli and Pizzuti\cite{1377172} introduced a method where an outlier is defined by taking into account of the sum of the distances from $1^st$ up to the $k^{th}$ nearest neighbors. These methods are sometimes referred as KNN in some papers. Note it is different from the term KNN in supervised machine learning. 

\textbf{Density-based approach} is another way to detect outlier on static data. The basic idea is to assign a degree of being outlier(a score) based on the density of local neighbourhood, given some predefined restrictions. A popular example of this approach is Local Outlier Factor(LOF) algorithm\cite{Breunig:2000:LID:342009.335388}, which is defined as follows: Let \textbf{k-distance(q)} denote as distance to the $k_{th}$ nearest neighbor of q, then

\begin{equation}
\textbf{reach-distk(q,p)} = max(d(q,p), k-distance(p))
\end{equation}

where d(q,p) is Euclidean distance from q to p  

\begin{equation}
\textbf{lrd(q)} =  \frac{1}{\sum\limits_{k \in knn(q)} reach-distk(q,p) / k}
\end{equation}

where \textbf{lrd(q)} is the local reachability density of data point q

And finally \textbf{LOF} of data point q is defined as:

\begin{equation}
\textbf{LOF(q)} = \frac{\frac{1}{k} \sum_{k \in knn(q)} lrd(q)}{lrd(q)}
\end{equation}

Data records (points) with high LOF have local densities smaller than their neighborhood and typically represent stronger outliers, unlike data points belonging to uniform clusters that usually tend to have lower LOF values\cite{4221341}

Both distance-based and density-based outlier detection techniques have a very wide range of applications, but one of their most notorious drawbacks is that it is usually computational expensive, especially in the scenario when the input data is high dimensional and a metric function is used to calculate the pair-wise distances.  

\textbf{Statistics approach} on the other hand, is another way to perform outlier detection on data with random access without requiring expensive computational resources. It is based on the probability theory and normally models the underlying data using a stochastic distribution(e.g. Gaussian distribution). One of the most popular one used is \textbf{auto-regression} model or sometimes being referred as Gaussian mixture model\cite{4438332}. It is very popular for time series outlier detection and its definition is given by 

\[ x(t) = a_{1}(t) \times x(t-1) + ... + a_{n}(t) \times x(t -n) + \xi(t) \]

where $\xi(t)$ is the noise and is almost always assumed to be a Gaussian white noise. Based on this formula, we can estimate the coefficient parameters $a_{i}(t)$ based on the given time series of $x(t),...x(t-n)$. The model can then be used to predict future time series by defining a threshold, called cut-off limit and the data point is identified as an outlier if it is beyond this threshold.

\textbf{Deviation} is another way of statically outlier detection first introduced by Arning\cite{A.Arning:deviation}, where an outlier is detected if feature space of one data point deviates largely from other data points. Aggarwal and YU\cite{Aggarwal:2001:ODH:375663.375668} proposed a technique for outlier detection. The basic idea in their definition is, a point is an outlier, if in some lower dimensional projection it is present in a local region of abnormally low density. This method is also an efficient method for high dimensional data set\cite{04666541}. Another technique introduced by Harkins\cite{S.Harkins:rnn} is to take advantage of replicator neutral network(RNN) to detect outliers. There might be other techniques used for unsupervised outlier detection but due to the limitation of this paper, I can not list all of them. The ones mentioned above are those best-known so far.

As modern applications have an increasing demands to process streaming data in real-time, a lot of these static methods mentioned before have been extended to work in the dynamic environments. The all based on the same ideas in static approaches but algorithms have been modified in an incremental fashion to address the \textbf{concept drift} of the data stream properties. Throughout this paper, these techniques are what I would be interested to apply in the world of parallel computation.



Clustering is another technique that are borrowed from machine learning to outlier detection. Two main algorithms exists for clustering-based approaches. One of them is called K-Mean clustering\cite{4666541}. It divides the data into a given $k$ clusters and the \emph{mean} of each clusters is calculated by metrics information of all data points in a cluster. If a data point is too far from the mean of its data point by a threshold, it will be considered as a outlier. Compared to K-Mean clustering, K-Median clustering\cite{DBLP:journals/corr/abs-1002-4003} clusters each chunk of data into variable number of clusters(from $k$ to $k log(n)$), and the \emph{weighted medians} found during clustering is considered for outlierness. Both of these two approaches will require users' input of value $k$ but K-Median clustering gives little bit more variation. 


\begin{enumerate}
  \item Distance-based approaches
  \item Statistical-based approaches
  \item Clustering approaches
  \item Deviation and other approaches
\end{enumerate}

Among all of these approaches, distance-based would probably be the most popular one. It was first introduced by Knorr and Ng\cite{EKnorr:1998}. The definition of it consists of the following:

% ############################################################################
% Bibliography
% ############################################################################
\bibliographystyle{plain}
\bibliography{my-bibliography}     %loads my-bibliography.bib

% ============================================================================
\end{document}
% ============================================================================
